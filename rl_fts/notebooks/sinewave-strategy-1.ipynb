{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "\n",
    "from strategies.sinewave import strategy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 12:04:18,412\tWARNING worker.py:523 -- `ray.get_gpu_ids()` will always return the empty list when called from the driver. This is because Ray does not manage GPU allocations to the driver process.\n",
      ":task_name:bundle_reservation_check_func\n",
      "2022-05-27 12:04:18,414\tINFO trial_runner.py:803 -- starting PPOTrainer_TradingEnv_5f2be_00000\n",
      ":actor_name:PPOTrainer\n",
      "2022-05-27 12:04:18,465\tINFO trainer.py:2295 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2022-05-27 12:04:18,466\tINFO ppo.py:268 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting simple_optimizer=True if this doesn't work for you.\n",
      "2022-05-27 12:04:18,466\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      ":actor_name:RolloutWorker\n",
      "2022-05-27 12:04:18,509\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-05-27 12:04:18,509\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":task_name:bundle_reservation_check_func\n",
      ":actor_name:PPOTrainer\n",
      ":actor_name:RolloutWorker\n",
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":actor_name:RolloutWorker\n",
      "2022-05-27 12:04:19,084\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-05-27 12:04:19,086\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":actor_name:RolloutWorker\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 12:04:20,091\tWARNING util.py:60 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, episode_reward_mean: -5.5570813464368305\n",
      "iteration: 2, episode_reward_mean: 26.59337160287309\n",
      "iteration: 3, episode_reward_mean: 25.5478782885347\n",
      "iteration: 4, episode_reward_mean: 36.32051227100865\n",
      "iteration: 5, episode_reward_mean: 43.46707561123379\n",
      "iteration: 6, episode_reward_mean: 51.321667986772155\n",
      "iteration: 7, episode_reward_mean: 58.8764069332023\n",
      "iteration: 8, episode_reward_mean: 64.18825797846615\n",
      "iteration: 9, episode_reward_mean: 68.67864979481308\n",
      "iteration: 10, episode_reward_mean: 72.47116045779347\n",
      "iteration: 11, episode_reward_mean: 76.01629300774425\n",
      "iteration: 12, episode_reward_mean: 80.93566762135623\n",
      "iteration: 13, episode_reward_mean: 83.43896675306473\n",
      "iteration: 14, episode_reward_mean: 87.24962808042395\n",
      "iteration: 15, episode_reward_mean: 90.85336415561831\n",
      "iteration: 16, episode_reward_mean: 94.51663828090314\n",
      "iteration: 17, episode_reward_mean: 99.2355412075789\n",
      "iteration: 18, episode_reward_mean: 107.81936611799736\n",
      "iteration: 19, episode_reward_mean: 115.4884665661025\n",
      "iteration: 20, episode_reward_mean: 123.47568050597368\n",
      "iteration: 21, episode_reward_mean: 129.62191625932016\n",
      "iteration: 22, episode_reward_mean: 135.11117739383164\n",
      "iteration: 23, episode_reward_mean: 139.505930681094\n",
      "iteration: 24, episode_reward_mean: 143.001844853211\n",
      "iteration: 25, episode_reward_mean: 145.95392200884424\n",
      "iteration: 26, episode_reward_mean: 149.39055755419218\n",
      "iteration: 27, episode_reward_mean: 153.28912910906712\n",
      "iteration: 28, episode_reward_mean: 156.8833176640307\n",
      "iteration: 29, episode_reward_mean: 159.53416954704338\n",
      "iteration: 30, episode_reward_mean: 162.16232070389594\n",
      "iteration: 31, episode_reward_mean: 163.31489575058438\n",
      "iteration: 32, episode_reward_mean: 165.5060294502086\n",
      "iteration: 33, episode_reward_mean: 167.2959554746261\n",
      "iteration: 34, episode_reward_mean: 168.36440714253487\n",
      "iteration: 35, episode_reward_mean: 169.1684063411953\n",
      "iteration: 36, episode_reward_mean: 169.29117625256995\n",
      "iteration: 37, episode_reward_mean: 168.68799555782962\n",
      "iteration: 38, episode_reward_mean: 169.02055116960545\n",
      "iteration: 39, episode_reward_mean: 169.80793532023208\n",
      "iteration: 40, episode_reward_mean: 171.01148902548087\n",
      "iteration: 41, episode_reward_mean: 172.00027751142136\n",
      "iteration: 42, episode_reward_mean: 173.5230977393915\n",
      "iteration: 43, episode_reward_mean: 174.8256735846871\n",
      "iteration: 44, episode_reward_mean: 176.069033057688\n",
      "iteration: 45, episode_reward_mean: 176.30964517302914\n",
      "iteration: 46, episode_reward_mean: 177.76389904014925\n",
      "iteration: 47, episode_reward_mean: 179.13883969508674\n",
      "iteration: 48, episode_reward_mean: 181.4223360026949\n",
      "iteration: 49, episode_reward_mean: 183.5566337602952\n",
      "iteration: 50, episode_reward_mean: 184.513998556084\n",
      "iteration: 51, episode_reward_mean: 186.63634656115758\n",
      "iteration: 52, episode_reward_mean: 188.24979831930622\n",
      "iteration: 53, episode_reward_mean: 190.35514788311815\n",
      "iteration: 54, episode_reward_mean: 192.7748699559301\n",
      "iteration: 55, episode_reward_mean: 194.06717766174842\n",
      "iteration: 56, episode_reward_mean: 195.6103615019599\n",
      "iteration: 57, episode_reward_mean: 197.51672197197058\n",
      "iteration: 58, episode_reward_mean: 199.67103180643798\n",
      "iteration: 59, episode_reward_mean: 201.2222194281215\n",
      "iteration: 60, episode_reward_mean: 201.8719147423095\n",
      "iteration: 61, episode_reward_mean: 201.89091790669147\n",
      "iteration: 62, episode_reward_mean: 202.4703904631832\n",
      "iteration: 63, episode_reward_mean: 203.19136557743877\n",
      "iteration: 64, episode_reward_mean: 203.91019137933748\n",
      "iteration: 65, episode_reward_mean: 203.9963832683158\n",
      "iteration: 66, episode_reward_mean: 204.15853667685354\n",
      "iteration: 67, episode_reward_mean: 205.30345234792858\n",
      "iteration: 68, episode_reward_mean: 206.39675951641246\n",
      "iteration: 69, episode_reward_mean: 207.3679907107611\n",
      "iteration: 70, episode_reward_mean: 208.56712627786968\n",
      "iteration: 71, episode_reward_mean: 210.39800708892156\n",
      "iteration: 72, episode_reward_mean: 212.41092486403798\n",
      "iteration: 73, episode_reward_mean: 214.2717535147771\n",
      "iteration: 74, episode_reward_mean: 215.6464827615668\n",
      "iteration: 75, episode_reward_mean: 216.75204231465727\n",
      "iteration: 76, episode_reward_mean: 218.23540749223343\n",
      "iteration: 77, episode_reward_mean: 220.16698540291952\n",
      "iteration: 78, episode_reward_mean: 222.8265281475546\n",
      "iteration: 79, episode_reward_mean: 225.2004259588182\n",
      "iteration: 80, episode_reward_mean: 227.0916999081117\n",
      "iteration: 81, episode_reward_mean: 229.50581557484864\n",
      "iteration: 82, episode_reward_mean: 231.58054928720696\n",
      "iteration: 83, episode_reward_mean: 232.99924825933616\n",
      "iteration: 84, episode_reward_mean: 234.02585873910877\n",
      "iteration: 85, episode_reward_mean: 235.04027528263407\n",
      "iteration: 86, episode_reward_mean: 235.93969555081134\n",
      "iteration: 87, episode_reward_mean: 236.65834904957177\n",
      "iteration: 88, episode_reward_mean: 236.9610799288769\n",
      "iteration: 89, episode_reward_mean: 237.30331708453062\n",
      "iteration: 90, episode_reward_mean: 237.58474151084474\n",
      "iteration: 91, episode_reward_mean: 238.32866050213372\n",
      "iteration: 92, episode_reward_mean: 238.89264498848664\n",
      "iteration: 93, episode_reward_mean: 239.80502975418224\n",
      "iteration: 94, episode_reward_mean: 240.45541898535276\n",
      "iteration: 95, episode_reward_mean: 240.28146973959042\n",
      "iteration: 96, episode_reward_mean: 240.12710641102734\n",
      "iteration: 97, episode_reward_mean: 239.74951488532417\n",
      "iteration: 98, episode_reward_mean: 239.3237236969524\n",
      "iteration: 99, episode_reward_mean: 238.8315813502743\n",
      "iteration: 100, episode_reward_mean: 238.2716841233713\n",
      "iteration: 101, episode_reward_mean: 238.28610571722197\n",
      "iteration: 102, episode_reward_mean: 237.51295916786594\n",
      "iteration: 103, episode_reward_mean: 237.1482440312895\n",
      "iteration: 104, episode_reward_mean: 236.3342849153429\n",
      "iteration: 105, episode_reward_mean: 235.2655679230268\n",
      "iteration: 106, episode_reward_mean: 232.11617592795002\n",
      "iteration: 107, episode_reward_mean: 228.9246350634956\n",
      "iteration: 108, episode_reward_mean: 225.55776703796698\n",
      "iteration: 109, episode_reward_mean: 222.58539809885485\n",
      "iteration: 110, episode_reward_mean: 219.85120781870032\n",
      "iteration: 111, episode_reward_mean: 217.83690437566892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 15:08:03,935\tWARNING tune.py:650 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "2022-05-27 15:08:14,656\tERROR tune.py:697 -- Trials did not complete: [PPOTrainer_TradingEnv_5f2be_00000]\n",
      "2022-05-27 15:08:14,657\tWARNING tune.py:707 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trail log directory: /Users/jordandaubinet/Documents/Repositories/masters/masters-code/logs/sinewave/strategy1/strategy1/PPOTrainer_TradingEnv_5f2be_00000_0_2022-05-27_12-04-18\n",
      "Time taken: 11038.32 seconds.\n"
     ]
    }
   ],
   "source": [
    "analysis = strategy1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy1.evaluate(analysis.best_trial.checkpoint.value)\n",
    "# strategy1.evaluate('/Users/jordandaubinet/Documents/Repositories/masters/masters-code/logs/sinewave/strategy1/strategy1/PPOTrainer_TradingEnv_7ab32_00000_0_2022-05-27_11-22-07/checkpoint_000020/checkpoint-20')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb3ccfb38e37e0c1755504f39bf6597091247ad6baa28346d5127bb05d3ae8ff"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('ray-tensortrade')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
