{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.oms.instruments import Instrument\n",
    "from tensortrade.env.default.actions import BSH\n",
    "from tensortrade.env.default.rewards import PBR\n",
    "from tensortrade.env import default\n",
    "from tensortrade.env.generic import Renderer\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "\n",
    "from tensortradeExtension.env.generic.components.renderer.positionChangeChart import PositionChangeChart\n",
    "\n",
    "from tensortrade.agents import DQNAgent\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "n_episodes = 10\n",
    "render_interval = 1000\n",
    "window_size = 30\n",
    "memory_capacity = n_steps * 10\n",
    "save_path = \"models/tests/sinewave/dqn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Instruments\n",
    "USD = Instrument(\"USD\", 2, \"U.S. Dollar\")\n",
    "TTC = Instrument(\"TTC\", 8, \"TensorTrade Coin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the rendere\n",
    "class PositionChangeChart(Renderer):\n",
    "\n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        p = list(history.price)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = p[i]\n",
    "                else:\n",
    "                    sell[i] = p[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"^\", color=\"green\")\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"red\")\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "\n",
    "        performance_df = pd.DataFrame().from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        performance_df.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data stream\n",
    "def generate_data_stream():\n",
    "  # generate x values over 1001\n",
    "  x = np.arange(0, 2*np.pi, 2*np.pi / 1001)\n",
    "  # generate y values from x values\n",
    "  y = 50*np.sin(3*x) + 100\n",
    "  # reset x values over 1000\n",
    "  x = np.arange(0, 2*np.pi, 2*np.pi / 1000)\n",
    "\n",
    "  price_stream = Stream.source(y, dtype=\"float\").rename(\"USD-TTC\")\n",
    "\n",
    "  return [price_stream, y]\n",
    "\n",
    "generate_data_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environement\n",
    "def create_env():\n",
    "    price_stream, y = generate_data_stream()\n",
    "    sinewaveex = Exchange(\"sine-wave\", service=execute_order)(\n",
    "        price_stream\n",
    "    )\n",
    "    cash = Wallet(sinewaveex, 1000 * USD)\n",
    "    asset = Wallet(sinewaveex, 0 * TTC)\n",
    "\n",
    "    portfolio = Portfolio(USD, [\n",
    "        cash,\n",
    "        asset\n",
    "    ])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        price_stream,\n",
    "        price_stream.rolling(window=10).mean().rename(\"fast\"),\n",
    "        price_stream.rolling(window=50).mean().rename(\"medium\"),\n",
    "        price_stream.rolling(window=100).mean().rename(\"slow\"),\n",
    "        price_stream.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(price=price_stream)\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=window_size,\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the batch size\n",
    "def get_optimal_batch_size(window_size=30, n_steps=1000, batch_factor=4, stride=1):\n",
    "    \"\"\"\n",
    "    lookback = 30          # Days of past data (also named window_size).\n",
    "    batch_factor = 4       # batch_size = (sample_size - lookback - stride) // batch_factor\n",
    "    stride = 1             # Time series shift into the future.\n",
    "    \"\"\"\n",
    "    lookback = window_size\n",
    "    sample_size = n_steps\n",
    "    batch_size = ((sample_size - lookback - stride) // batch_factor)\n",
    "    return batch_size\n",
    "\n",
    "batch_size = get_optimal_batch_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment details\n",
    "print(\"Action Space: \"+str(env.action_space))\n",
    "print(\"State Space: \"+str(env.observation_space))\n",
    "print(\"Next observation\")\n",
    "env.observer.feed.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = DQNAgent(env)\n",
    "\n",
    "# agent.train(\n",
    "#   batch_size=batch_size, \n",
    "#   n_steps=n_steps, \n",
    "#   n_episodes=n_episodes, \n",
    "#   memory_capacity=memory_capacity, \n",
    "#   render_interval=render_interval,\n",
    "#   save_path=save_path\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the environment\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.get_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action = agent.get_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
